{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27251e20",
   "metadata": {},
   "source": [
    "# Aim : To Study the problems generally faced in training an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7030b",
   "metadata": {},
   "source": [
    "1. Vanishing and Exploading Gradient\n",
    "- Gradient becomes very small\n",
    "- Gradient becomes very high\n",
    "- Results in Unstable gradients implies never reach good solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560ec3",
   "metadata": {},
   "source": [
    "2. Requires Lots of Data\n",
    "- Transfer Learning and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb7fe6",
   "metadata": {},
   "source": [
    "3. Slow training when network is learge\n",
    "- Better Optimizer and better Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9247744",
   "metadata": {},
   "source": [
    "4. Risk of overfitting\n",
    "* Cause: Millions of parameters (trainabile weights), Not Enough Data, Noicy Data (unwanted data)\n",
    "* Solution: Regularizations, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419fb32",
   "metadata": {},
   "source": [
    "## There are the list of problems we generally face while training a Neural Network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dece7",
   "metadata": {},
   "source": [
    "### 1. Vaishing and Exploading Gradient Problems :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fbec5",
   "metadata": {},
   "source": [
    "<img src=\"images/veg.jpg\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8a974",
   "metadata": {},
   "source": [
    "**Weight update formula of Gradient Discent is given by**\n",
    "$$W_{new} = W_{old}-\\eta\\frac{\\partial C}{ \\partial W_{old}}\\tag{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7f5e4",
   "metadata": {},
   "source": [
    "`Cast Function`\n",
    "$$\\because C = y-y\\hat{}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc952e9",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{1}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial a^1}\\cdot \\frac{\\partial a^1}{\\partial w_{1}}\\tag{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730b7c8",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{2}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial w_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721dae9",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{3}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial w_{3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a0a68",
   "metadata": {},
   "source": [
    "**As you can see the rate of change of cost function with respect of weights depends on : `the derivative of activation function`**\n",
    "\n",
    "**Let's look at the sigmoid activation funciton**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b4d79",
   "metadata": {},
   "source": [
    "$$Sigmoid \\left(x\\right) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ff9fb",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb83cf5",
   "metadata": {},
   "source": [
    "**Derivative of Sigmoid Function**\n",
    "$$Sigmoid^{`} \\left(x\\right) = \\frac{e^{-x}}{(1+e^{-x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854386f",
   "metadata": {},
   "source": [
    "<img src=\"images/dsigmoid.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15626088",
   "metadata": {},
   "source": [
    "`As you can see the derivative of sigmoid function is maximum at 0 and the maximum vaue is 0.25.`\n",
    "\n",
    "Have a look at equation (ii) now, if the values of 0.25 * 0.25 * 0.25 = 0.015625. This value after miltiply by learning becomes even lesser also decreases drastically as we increase the layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d858e",
   "metadata": {},
   "source": [
    "`Now if you look at equation (i), weight update becomes negligable as we increase the layer and we never rech to the solution` This is called as **Vanishing Gradient Problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285b94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
