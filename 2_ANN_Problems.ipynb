{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27251e20",
   "metadata": {},
   "source": [
    "# Aim : Do Study the problems generally faced in training an ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7030b",
   "metadata": {},
   "source": [
    "1. Vanishing and Exploading Gradient\n",
    "- Gradient becomes very small\n",
    "- Gradient becomes very high\n",
    "- Results in Unstable gradients implies never reach good solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560ec3",
   "metadata": {},
   "source": [
    "2. Requires Lots of Data\n",
    "- Transfer Learning and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb7fe6",
   "metadata": {},
   "source": [
    "3. Slow training when network is learge\n",
    "- Better Optimizer and better Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9247744",
   "metadata": {},
   "source": [
    "4. Risk of overfitting\n",
    "* Cause: Millions of parameters (trainabile weights), Not Enough Data, Noicy Data (unwanted data)\n",
    "* Solution: Regularizations, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32139d81",
   "metadata": {},
   "source": [
    "## There are the list of problems we generally face while training a Neural Network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9bcfa",
   "metadata": {},
   "source": [
    "### 1. Vaishing and Exploading Gradient Problems :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b9924",
   "metadata": {},
   "source": [
    "<img src=\"images/veg.jpg\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373d2f6",
   "metadata": {},
   "source": [
    "**Weight update formula of Gradient Discent is given by**\n",
    "$$W_{new} = W_{old}-\\eta\\frac{\\partial C}{ \\partial W_{old}}\\tag{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde07fb2",
   "metadata": {},
   "source": [
    "$$\\because C = y-y\\hat{}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaf5c9",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{1}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial a^1}\\cdot \\frac{\\partial a^1}{\\partial w_{1}}\\tag{ii}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39494af7",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{2}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial a^2}\\cdot \\frac{\\partial a^2}{\\partial w_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be9e93",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{\\partial C}{\\partial w_{3}} = \\frac{\\partial C}{\\partial y\\hat{}} \\cdot \\frac{\\partial y\\hat{}}{\\partial a^3}\\cdot \\frac{\\partial a^3}{\\partial w_{3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35505919",
   "metadata": {},
   "source": [
    "**As you can see the rate of change of cost function with respect of weights depends on : `the derivative of cast function`**\n",
    "\n",
    "**Below we take an example of sigmoid funciton**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edb043",
   "metadata": {},
   "source": [
    "$$Sigmoid \\left(x\\right) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c6187",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ecfbfb",
   "metadata": {},
   "source": [
    "**Derivative of Sigmoid Function**\n",
    "$$Sigmoid^{`} \\left(x\\right) = \\frac{e^{-x}}{(1+e^{-x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0dfc4f",
   "metadata": {},
   "source": [
    "<img src=\"images/dsigmoid.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f041e71",
   "metadata": {},
   "source": [
    "`As you can see the derivative of sigmoid function is maximum at 0 and the maximum vaue is 0.25.`\n",
    "\n",
    "Have a look at equation (i) now, if the values of 0.25 * 0.25 * 0.25 = 0.015625. This value after miltiply by learning becomes even lesser also decreases drastically as we increase the layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66bdd1",
   "metadata": {},
   "source": [
    "`Now if you look at equation (i), weight update becomes negligable as we increase the layer.` This is **Vanishing Gradient Problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace192d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
