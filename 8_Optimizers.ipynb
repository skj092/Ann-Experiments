{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7de84f",
   "metadata": {},
   "source": [
    "# Aim : To Compare different available `Optimizers` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e12314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\users\\sonu.ramkumar.jha\\desktop\\experiments\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.python.keras import activations\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# downloading fashion_mnist data\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf87d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "activation = tf.keras.activations.tanh\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "tf.keras.layers.Dense(128, activation=activation),\n",
    "tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "optim = tf.keras.optimizers.SGD()\n",
    "model.compile(optimizer=optim,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a53b0",
   "metadata": {},
   "source": [
    "<img src=\"images\\model.jpeg\" height=50% width=50% alt-text=\"Case 1 Gradient Descent\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72e889",
   "metadata": {},
   "source": [
    "**Forward Pass:**\n",
    "\n",
    "$$\\overbrace{\\begin{bmatrix}\n",
    "    i_{1}  \\\\\n",
    "    i_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    i_{784}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    w_{11} & x_{12} & x_{13} & \\dots  & x_{1128} \\\\\n",
    "    w_{21} & x_{22} & x_{23} & \\dots  & x_{2128} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{7841} & x_{7842} & x_{7843} & \\dots  & x_{784128}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    b_{1}  \\\\\n",
    "    b_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    b_{128}\n",
    "\\end{bmatrix}}^{Input of First Activation Function} =\\begin{bmatrix}\n",
    "    s_{1}  \\\\\n",
    "    s_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    s_{128}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$Activation1 \\ \\left (\\begin{bmatrix}\n",
    "    s_{1}  \\\\\n",
    "    s_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    s_{128}\n",
    "\\end{bmatrix} \\right)=\\begin{bmatrix}\n",
    "    i^{`}_{1}  \\\\\n",
    "    i^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    i^{`}_{128}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\overbrace{\\begin{bmatrix}\n",
    "    i^{`}_{1}  \\\\\n",
    "    i^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    i^{`}_{128}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    w^{`}_{11} & w^{`}_{12} & w^{`}_{13} & \\dots  & w^{`}_{110} \\\\\n",
    "    w^{`}_{21} & w^{`}_{22} & w^{`}_{23} & \\dots  & w^{`}_{210} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{`}_{1281} & w^{`}_{1282} & w^{`}_{1283} & \\dots  & w^{`}_{12810}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    b^{`}_{1}  \\\\\n",
    "    b^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    b^{`}_{128}\n",
    "    \\end{bmatrix}}^{Input of Second Activation Function}=\\begin{bmatrix}\n",
    "    s^{`}_{1}  \\\\\n",
    "    s^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    s^{`}_{10}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$Activation2 \\ \\left (\\begin{bmatrix}\n",
    "    s^{`}_{1}  \\\\\n",
    "    s^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    s^{`}_{128}\n",
    "\\end{bmatrix}\\  \\right )=\\begin{bmatrix}\n",
    "    y^{`}_{1}  \\\\\n",
    "    y^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    y^{`}_{10}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e7d1f",
   "metadata": {},
   "source": [
    "**Whole Forward Pass:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904989e",
   "metadata": {},
   "source": [
    "$$Activation2\\left (Activation1\\left (\\overbrace{\\begin{bmatrix}\n",
    "    i_{1}  \\\\\n",
    "    i_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    i_{784}\n",
    "\\end{bmatrix}*\\begin{bmatrix}\n",
    "    w_{11} & x_{12} & x_{13} & \\dots  & x_{1128} \\\\\n",
    "    w_{21} & x_{22} & x_{23} & \\dots  & x_{2128} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{7841} & x_{7842} & x_{7843} & \\dots  & x_{784128}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    b_{1}  \\\\\n",
    "    b_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    b_{128}\n",
    "\\end{bmatrix}}^{Input of Hidden Layer}  \\right )+\\begin{bmatrix}\n",
    "    w^{`}_{11} & w^{`}_{12} & w^{`}_{13} & \\dots  & w^{`}_{110} \\\\\n",
    "    w^{`}_{21} & w^{`}_{22} & w^{`}_{23} & \\dots  & w^{`}_{210} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w^{`}_{1281} & w^{`}_{1282} & w^{`}_{1283} & \\dots  & w^{`}_{12810}\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    b^{`}_{1}  \\\\\n",
    "    b^{`}_{2}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    b^{`}_{128}\n",
    "    \\end{bmatrix}\\  \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c0e2e",
   "metadata": {},
   "source": [
    "**In Simple Form**\n",
    "\n",
    "$$Activation2 \\ (Activation1 \\ (I_{1784} \\ W_{784128}+B_{128})+W^{`}_{12810}+B{`}_{10}) = y{`}$$\n",
    "\n",
    "**Cast Function(Error):**\n",
    "$$C = (y-y{`})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57367f",
   "metadata": {},
   "source": [
    "**Backword Pas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b59cc",
   "metadata": {},
   "source": [
    "**We have 4 values to update before the 2nd forward pass -** \n",
    "\n",
    "$$\\vec W, \\vec B, \\vec W{`} and \\ \\vec B{`}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb5f0d",
   "metadata": {},
   "source": [
    "## Gradient Discent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a654ad3",
   "metadata": {},
   "source": [
    "**Gradien Discent Formula**\n",
    "\n",
    "$$\\boxed{\\vec{W_{new}} = \\vec{W_{old}} - \\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{old}}}}\\$$\n",
    "\n",
    "**Where**\n",
    "\n",
    "\\begin{equation}\n",
    " \\left.\\begin{aligned}\n",
    "        \\vec{W_{new}} = New \\ Weight\\\\\n",
    "        \\vec{W_{old}} = Old \\ Weight\\\\\n",
    "        \\eta = learning \\ rate\n",
    "       \\end{aligned}\n",
    " \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**According to Chain Rule**\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\vec C}{\\partial \\vec W} = \\frac{\\partial C}{\\partial y{`}}\\times\\frac{\\partial y{`}}{\\partial Activation2}\\times\\frac{\\partial Activation2}{\\partial Activation1}\\times \\frac{\\partial Activation1}{\\partial W}\\\\\n",
    "\\frac{\\partial \\vec C}{\\partial \\vec B} = \\frac{\\partial C}{\\partial y{`}}\\times\\frac{\\partial y{`}}{\\partial Activation2}\\times\\frac{\\partial Activation2}{\\partial Activation1}\\times\\frac{\\partial Activation1}{\\partial B}\\\\\n",
    "\\frac{\\partial \\vec C}{\\partial \\vec W{`}} = \\frac{\\partial C}{\\partial y{`}}\\times\\frac{\\partial y{`}}{\\partial Activation2}\\times\\frac{\\partial Activation2}{\\partial W_{`}}\\\\\n",
    "\\frac{\\partial \\vec C}{\\partial \\vec B{`}} = \\frac{\\partial C}{\\partial y{`}}\\times\\frac{\\partial y{`}}{\\partial Activation2}\\times\\frac{\\partial Activation2}{\\partial B_{`}}\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07bf6e",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "    - As you can see Wn is directly propotional to Dc/Dw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8586fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_images, train_labels, epochs=10, batch_size=32)\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "# print('test_loss', test_loss)\n",
    "# print('test_accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d16a85",
   "metadata": {},
   "source": [
    "# Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8fbe50",
   "metadata": {},
   "source": [
    "**With momentum the weight update formula becomes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ebac9",
   "metadata": {},
   "source": [
    "$$\\vec{W_{new}} = \\vec{W_{old}} - m_{new}$$\n",
    "$$m_{new} = \\beta \\ m_{old}+\\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{old}}}$$\n",
    "\n",
    "`Where m is called momentum and` \n",
    "\n",
    "$$\\beta : coefficient \\ of \\ momentum$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71ad92",
   "metadata": {},
   "source": [
    "**Step:1**\n",
    "$$When: \\ \\vec W_{old} = W_{0} \\ and \\ m = m_{0} = 0$$\n",
    "\n",
    "$$\\therefore m_{1} = \\beta \\ m_{0}+\\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}\\$$\n",
    "\n",
    "$$\\therefore m_{1} = \\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}} \\ \\left (\\because m_{0}=0\\right )\\tag{i}$$\n",
    "\n",
    "$$\\therefore \\boxed{\\vec W_{1} = W_{0}-m_{1} = \\vec W_{0} - \\eta \\frac{\\partial \\vec\n",
    "{C}}{\\partial \\vec{W_{0}}}}\\tag{Same as Gradient Discent Formula}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee424e86",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- As you can see for the first time weight update happens like the `Gradient Discent` when `m=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce4136",
   "metadata": {},
   "source": [
    "**Step:2**\n",
    "$$When: \\ \\vec W_{old} = W_{1} \\ and \\ m =  m_{1} = \\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}\\tag{from eq(i)}\\$$\n",
    "\n",
    "$$\\therefore \\vec m_{2} = \\beta \\ m_{1} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}\\tag{ii}$$\n",
    "$$\\therefore \\vec W_{2} = \\vec W_{1} - m2$$\n",
    "$$\\therefore \\vec W_{2} = \\vec W_{1} - \\beta \\ m_{1} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}$$\n",
    "$$\\therefore \\vec W_{2} = \\vec W_{1} - \\beta \\left( \\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}} \\right)+ \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}$$\n",
    "$$\\therefore \\boxed{\\vec W_{2} = \\vec W_{1} - \\eta \\left( \\beta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}+\\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}} \\right)}$$\n",
    "$$For \\ \\beta = 0.9$$\n",
    "$$\\therefore \\boxed{\\vec W_{2} = \\vec W_{1} - \\eta \\left( 0.9 \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}+\\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}} \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd8734",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- For the second time weight depends on 90% of the initial weight plus 100% last weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16c51d",
   "metadata": {},
   "source": [
    "**Step:3**\n",
    "$$When: \\ \\vec W_{old} = W_{2} \\ and \\ m =  \\vec m_{2} = \\beta \\ m_{1} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}\\tag{from eq(ii)}$$\n",
    "\n",
    "$$\\therefore \\vec m_{3} = \\beta \\ m_{2} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}}\\tag{iii}$$\n",
    "$$\\therefore \\vec m_{3} = \\beta \\left( \\beta \\ m_{1} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}\\right) + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}}$$\n",
    "$$\\therefore \\vec m_{3} = \\beta \\left( \\beta \\ \\eta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}} + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}\\right) + \\eta \\ \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}}\\tag{from eq(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830bf39",
   "metadata": {},
   "source": [
    "$$\\therefore \\boxed{ \\vec m_{3} = \\eta \\left (\\beta^{2} \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}+ \\beta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}+\\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}} \\right)}\\tag{iv}$$\n",
    "$$\\therefore \\vec W_{3} = \\vec W_{2}-m_{3}$$\n",
    "$$\\therefore \\boxed{\\vec W_{3} = \\vec W_{2}-\\eta \\left (\\beta^{2} \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}+ \\beta \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}+\\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}} \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc12d4",
   "metadata": {},
   "source": [
    "**Case:1**\n",
    "$$For \\ \\beta = 0$$\n",
    "$$\\vec W_{3} = \\vec W_{2} - \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}}\\tag{Same as Gradiend Discent}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac6bbb",
   "metadata": {},
   "source": [
    "**Case:2**\n",
    "$$For \\ \\beta = 0.9$$\n",
    "$$\\therefore \\boxed{\\vec W_{3} = \\vec W_{2}-\\eta \\left (0.81 \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{0}}}+ 0.9 \\frac{\\partial \\vec{C}}{\\partial \\vec{W_{1}}}+\\frac{\\partial \\vec{C}}{\\partial \\vec{W_{2}}} \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fdcfe0",
   "metadata": {},
   "source": [
    "**Observatinos**\n",
    "- As you can see in Case:1 for beta = 0, weight update happens like Gradient Discent.\n",
    "- For beta=0.9 (practically good), Weight depends on 81% of past weight, 90% recent past weight and 100% the last weight.\n",
    "- Therefore `momemtum optimizer` has and advantage over `Gradient Discent` that with the `impace of previous weight` it can jump through `saddle point`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation = tf.keras.activations.tanh\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "# tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "# tf.keras.layers.Dense(128, activation=activation),\n",
    "# tf.keras.layers.Dense(10)\n",
    "# ])\n",
    "\n",
    "# optim = tf.keras.optimizers.SGD()\n",
    "# model.compile(optimizer=optim,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "\n",
    "# # model summary\n",
    "# model.summary()\n",
    "\n",
    "# model.fit(train_images, train_labels, epochs=10, batch_size=32)\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "# print('test_loss', test_loss)\n",
    "# print('test_accuracy', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534f1ae",
   "metadata": {},
   "source": [
    "## 3. Nesterov Accelaraged Gradient (NAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7e85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
